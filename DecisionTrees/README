README for 600.335 assignment 4 Decision Trees

README -- description document.

compile.sh -- file to compile the java code.

run_traditional.sh -- bash script to run the classic decision tree classifier

run_evolutionary.sh -- bash script used to run the genetic decision tree classifier

src/ -- source code folder
  Makefile -- File used to compile the code

  data_splitter.py -- script to split data sets into training sets and test sets
			required for the mushroom and voting record data sets.
			The script splits the data with 70% in training and
			the other 30% in the test set.

			It is run from the top level directory by calling:

	python src/data_splitter.py <input_file> <training file> <testing file>


  edu/jhu/cs/wilkes/cs335/assignment4/ -- high level package for the source code.
    CaseForClassification.java -- class for a case to be classified. Splits up
 				attributes and the classification.

    DecisionTreeClassifier.java -- main class for the decision
				tree classifiers.  Reads in the data files,
				parses them, and then creates a decision tree
				and classifies a data set.
	
	IdentifierProbabilityPair.java -- class for matching the probability of an
				attribute with the value of that attribute
				for storage in a priority queue.

    IdentifierProbabilities.java -- data structure for storing probability 
				pairs, and for looking up the most common value
				of each attribute, as well as the probability
				of each attribute value.
	
	AttributeInfo.java -- data structrue for storing the numbers of values for each
				discovered attribute and for accessing the different attribute	
				values seen during testing. 
	
	DecisionTree.java -- Interface for a decision tree. specifies the classify
				method to classify cases.
	
  edu/jhu/cs/wilkes/cs335/assignment4/traditional/ -- folder containing source
	code for a traditional information gain based decision tree.
  
    TraditionalDecisionTreeNode.java -- Class for representing decision tree nodes.
				handles both the creation of the decision
				tree recursively, as well as the classification
				of test cases using an information gain based strategy.
  edu/jhu/cs/wilkes/cs335/assignment4/genetic -- folder containing source code for
				a genetic decision tree classifier
	
	GeneticTree.java -- Wrapper class for the GeneticTreeNode.java.  Performs
				the operations required for genetically updating the tree, or in
				other words the crossover, mutation, and selection of children for
				each iteration.

	GeneticTreeNode.java -- Class which stores the data about each node in the tree.
				Contains info about decision variable, as well as its children. Supports
				the classification of individual cases.


  doc/ -- directory containing relevant documentation, including JavaDocs.

  data/ -- directory containing input files. Some of these files have been
		produced by running the data_split.py script, others have not.
		training and testing pairs are listed below. Training/Testing 
		files must be either comma separated value, or have spaces
		between values.  The classificaiton must be the first value,
		and all others shall be the attributes of that case.

	The following are training and testing pairs:
	house_votes_training.data 	house_votes_testing.data
	monks-1.train				monks-1.test
	monks-2.train				monks-2.test
	monks-3.train				monks-3.test
	mushroom_training.txt		mushroom_tesing.txt


  output/ -- a directory containing the output of various runs of the program.
		The runs will be named based on their data. There are also a 
		fairly large collection of .csv files which are the stripped
		down output of running the code on a few cases at once to
		be used for the various experiments I have run.	

		For each of the data sets provided, the code should take no more
		than 30 seconds to run.

  bin/ -- a directory for .class files and other compiled code.

How to run the code:
In order to run the code, first it must be compiled.  This can be achieved by
running the following command from the top level directory:

./compile.sh

Once the code is compiled, to run enter the following style command:

./run_traditional.sh <path to training data> <path to test data>

or

./run_evolutionary.sh <path to training data> <path to test data>

The classifier will then output data about the amount of time required to 
build the training set, the performance of classifying the training set,
then the performance of classifying the testing set.

Performance:

The standard decision tree showed very good performance as is, to me indicating
a bug.  It only has false classifications on the house voting records, on all
others it perfectly classifies the whole test set, but it does so very quickly. 

I have also implemented a method to use the gain ratio instead of the simple information
gain.  This is not used by default, as the information gain seemed to work perfectly
fine without it. 

For the genetic algorithm, I have implemented roughly the tree described in the 
assignment document.  I have used a tree based encoding, with each node containing
an attribute to switch on, and each of its children is originally one of the values
of that attribute.  I have implemented a few different fitness functions, but the one
which seems to work best is the sum of the accuracy, precision and recall of the
classifications of the training set, with the average depth of the tree scaled
by a constant factor subtracted from this value.

For the crossover I have implemented a method which takes two trees from the current
set and pulls a node from each. Both the first and second trees have random 
nodes selected from them, and then the node from the second tree replaces the
original node from the first tree.  The second tree is then discarded, and the 
first tree is added as a child to the next generation.    

Mutations are achieved by picking a random node in a tree and swapping its 
attribute which its children switch on with another attribute which has the same
number of possibilities from the training set.  If the node selected does not
have any new attributes it could take on, another node is selected randomly until
either the algorithm has tried five times, or a suitable replacement is found.  

My current implementation has all trees from one generation continuing on to
the next except for those who are used as the  second tree in crossover, which
are discarded, and replaced by a new random tree.   This way, there is an element of
random restarts, but the trees evolve genetically between generations.

Reflections:
Both of the classifiers were fairly straightforward to build, but unfortunately due
to other commitments for other projects, I was unable to do as much experimentation
with the genetic tree as I would have liked, especially for the selection strategies.
Also, due to the near perfect performance of my traditional classifier, and that it is
much faster than the genetic classifier, it doesn't seem like GAs help very much.

